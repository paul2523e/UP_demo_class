{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple notebook showing overfitting for higher model complexity.\n",
    "\n",
    "A polinomial fit is evidently not a good model, but it is used because of its mathematical simplicity\n",
    "\n",
    "(Bishop uses a similar approach to show bias-variance trade-off and/or overfitting)\n",
    "\n",
    "Prepared for a demo class in a peruvian university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ceb9b24ac34dc2ae2417f90d53aa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='Model Complexity (degree)', layout=Layout(margin='0 0 0 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_polynomial_fit(degree, test_visible)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from ipywidgets import interact,IntSlider,Layout,VBox,Checkbox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MAX_DEGREE = 6\n",
    "NOISE_LEVEL = 0.4\n",
    "N_POINTS = int(2*MAX_DEGREE)\n",
    "TEST_SIZE = 0.4\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def plot_polynomial_fit(degree,test_visible):\n",
    "    # reproducibility:\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    #domain:\n",
    "    min_x,max_x = -1.5,1.5\n",
    "    x = np.linspace(min_x, max_x, N_POINTS)\n",
    "\n",
    "    # real function:\n",
    "    def fun(x):\n",
    "        return  0.6*(x+0.1)**3 + np.random.normal(0, NOISE_LEVEL, x.shape)\n",
    "    \n",
    "    y = fun(x)\n",
    "    # train test splitting and output evaluation for synthetic data:\n",
    "    # to avoid extrapolation, make sure first and last points are in train:\n",
    "    # this is just to focus around the point intended in the class:\n",
    "    indices = range(N_POINTS)\n",
    "    \n",
    "    # Split using indices\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=TEST_SIZE, random_state=SEED)\n",
    "    #make sure that first and last points are in the training set so that we are not extrapolating:\n",
    "    train_idx += [0,N_POINTS-1]\n",
    "    train_idx = list(set(train_idx))\n",
    "    # if either first or last index has been moved to train set, make sure they \n",
    "    # are not in the test set then:\n",
    "    test_idx = [v for v in test_idx if v not in train_idx]\n",
    "    # Use the indices to get the train and test sets\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    degree_range = (1, MAX_DEGREE, 1) #start,end and step\n",
    "\n",
    "    # do a pass just to get min and max for y axis to keep points constants in the canvas later:\n",
    "    # kept separate to improve readabilty of program:\n",
    "    min_y = min(min(y_train),min(y_test))\n",
    "    max_y = max(max(y_train),max(y_test))\n",
    "    for d in range(degree_range[0],degree_range[1]+1,degree_range[2]):\n",
    "        # Fit polynomial\n",
    "        coeffs,residuals, _, _, _  = np.polyfit(x_train, y_train, d,full=True,rcond=np.finfo(float).eps/10)\n",
    "        poly_func= np.poly1d(coeffs)\n",
    "        x1 = np.linspace(min_x, max_x, 5*N_POINTS)\n",
    "        y1 = poly_func(x1)\n",
    "        min_y,max_y = min(min_y,min(y1)),max(max_y,max(y1))\n",
    "  \n",
    "    # fits mapping:\n",
    "    fits_dict = {}\n",
    "    train_MSE_dict = {}\n",
    "    test_MSE_dict = {}\n",
    "    colors = []\n",
    "    # build fits:\n",
    "    \n",
    "\n",
    "    # Create plot\n",
    "    # Create a figure with 1 row and 2 columns\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    plt.suptitle('UP demo class - 12-Feb-25 - © Paul Arriz Tisoc', fontsize=16, fontfamily=\"serif\", fontweight=\"bold\")\n",
    "\n",
    "    # Define grid spec: first subplot will take more space (e.g. 2/3 of the width)\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])  # The first subplot is twice as wide as the second\n",
    "\n",
    "    # Create subplots with the custom grid spec\n",
    "    axs = [plt.subplot(g) for g in gs]\n",
    "\n",
    "    # iteract of ipywidgets includes last element but range doesnt\n",
    "    # which casues with range below to be a little ugly:\n",
    "    for deg in range(degree_range[0],degree_range[1]+1,degree_range[2]):   \n",
    "\n",
    "        # Fit polynomial\n",
    "        coeffs,residuals, _, _, _  = np.polyfit(x_train, y_train, deg,full=True,rcond=np.finfo(float).eps/10)\n",
    "        \n",
    "        poly_func= np.poly1d(coeffs)\n",
    "        # append new fit:\n",
    "        fits_dict[deg] = poly_func\n",
    "        # per numpy: residuals – sum of squared residuals of the least squares fit\n",
    "        # residuals provided before as sum of squared values:\n",
    "        train_MSE_dict[deg] = np.sqrt(residuals)/len(x_train)\n",
    "\n",
    "        #the outputs calculated using the \"trained\" model\n",
    "        y_pred_train = poly_func(x_train)\n",
    "        y_pred_test = poly_func(x_test)\n",
    "        # the error between the real values for x_test, time N to make it sum\n",
    "        #as in redisuals\n",
    "        test_MSE_dict[deg] = mean_squared_error(y_true=y_test,y_pred=y_pred_test)\n",
    "        \n",
    "        if deg == degree:\n",
    "            colors.append('red')\n",
    "            #plot the given left side chart\n",
    "            #axs[0].scatter(x_train, y_train, label='Noisy Data')\n",
    "            axs[0].plot(x_train, y_train, 'x', color='magenta', markersize = 16, label='Train - Real Data')\n",
    "            axs[0].plot(x_train, y_pred_train, 'o', color='magenta', label='Train - Predicted')\n",
    "            if test_visible:\n",
    "                axs[0].plot(x_test, y_test, 'x', color='lightgreen',markersize = 16, label='Test  - Real Data')\n",
    "                axs[0].plot(x_test, y_pred_test, 'o', color='green', label='Test  - Predicted')\n",
    "            # plot the actual model including point inbeweeen:\n",
    "            x1 = np.linspace(min_x, max_x, 5*N_POINTS)\n",
    "            y1 = poly_func(x1)\n",
    "            axs[0].plot(x1, y1,'k--', label=f'Model fit (grado {deg})')\n",
    "\n",
    "            axs[0].set_title(f' A fit of some data points with some simplistic model')\n",
    "            axs[0].set_xlabel('x : The data we have as predictor(s) - the input to the model')\n",
    "            axs[0].set_ylabel('y : The Value we attempt to predict - the output of the model')\n",
    "\n",
    "\n",
    "        else:\n",
    "            colors.append('black')\n",
    "    # there are better ways of doing this, but this is simpler to explain:\n",
    "    # still part of the curve might be about of the boundaries\n",
    "\n",
    "    axs[0].set_ylim(1.1*min_y,1.1*max_y)\n",
    "\n",
    "    #plot the accuracies\n",
    "    axs[1].scatter(train_MSE_dict.keys(), train_MSE_dict.values(), color = colors, label='Train data - error')\n",
    "    axs[1].plot(train_MSE_dict.keys(), train_MSE_dict.values(),\"k--\")\n",
    "    if test_visible:\n",
    "        axs[1].scatter(test_MSE_dict.keys(), test_MSE_dict.values(), color = [c if c == 'red' else 'green' for c in colors], label='Test data - error')\n",
    "        axs[1].plot(test_MSE_dict.keys(), test_MSE_dict.values(),\"g--\")\n",
    "    axs[1].axvline(x=degree, color='red', linestyle='--', label='current complexity (grade)')\n",
    "    axs[1].set_title(f' Error of the model for different complexities')\n",
    "    axs[1].set_xlabel('<-- lower -- Model Complexity  -- higher-->')\n",
    "    axs[1].set_ylabel('Mean Squared Error')\n",
    "    axs[1].set_yscale(\"log\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    axs[0].legend(loc='upper left',)\n",
    "\n",
    "    #accuracy_score()\n",
    "\n",
    "    #fig.show()\n",
    "\n",
    "# Create interaction\n",
    "common_layout = Layout(width='600px', margin='0 0 0 0')\n",
    "interact(plot_polynomial_fit, \n",
    "         degree=IntSlider(description='Model Complexity (degree)', \n",
    "                          min=1, \n",
    "                          max=MAX_DEGREE, \n",
    "                          step=1, \n",
    "                          value=5, \n",
    "                          layout=common_layout,  # Set the overall width\n",
    "                          style={'description_width': '200px'},  # Increase the description width\n",
    "                          height='60px'),\n",
    "         test_visible=Checkbox(\n",
    "            description='Show test data',\n",
    "            value=False,\n",
    "            layout=common_layout,  # Set the overall width\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import fetch_california_housing\n",
    "#housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_df = pd.DataFrame(data = housing['data'],columns= housing['feature_names'])\n",
    "#y = pd.Series(data = housing['target'], name = housing['target_names'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#housing['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedHouseVal']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
